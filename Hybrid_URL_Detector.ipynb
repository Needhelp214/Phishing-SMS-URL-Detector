{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1 ‚Äî Install & Import"
      ],
      "metadata": {
        "id": "sIfyI3jUWDVr"
      },
      "id": "sIfyI3jUWDVr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LOgirM9VSz-"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y jax jaxlib > /dev/null 2>&1 || true\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, Bidirectional, LSTM,\n",
        "    Dense, Dropout, BatchNormalization, Concatenate, SpatialDropout1D\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(f\"‚úî Libraries loaded. TensorFlow Version: {tf.__version__}\")"
      ],
      "id": "2LOgirM9VSz-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extractor"
      ],
      "metadata": {
        "id": "BW4WTWUiWIgl"
      },
      "id": "BW4WTWUiWIgl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIqql5AkVSz-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_entropy(text):\n",
        "    if not text: return 0\n",
        "    length = len(text)\n",
        "    counts = Counter(text)\n",
        "    entropy = 0\n",
        "    for count in counts.values():\n",
        "        p = count / length\n",
        "        entropy -= p * math.log2(p)\n",
        "    return entropy\n",
        "\n",
        "def extract_features(url):\n",
        "    url = str(url)\n",
        "    return [\n",
        "        len(url),\n",
        "        url.count('.'),\n",
        "        url.count('-'),\n",
        "        url.count('@'),\n",
        "        url.count('?'),\n",
        "        url.count('&'),\n",
        "        url.count('='),\n",
        "        url.count('_'),\n",
        "        sum(c.isdigit() for c in url),\n",
        "        calculate_entropy(url),\n",
        "        1 if 'https' in url else 0,\n",
        "        1 if 'http' in url else 0,\n",
        "        1 if 'www' in url else 0,\n",
        "    ]\n",
        "\n",
        "print(\"‚úî Feature extraction functions ready\")"
      ],
      "id": "nIqql5AkVSz-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load CSVs"
      ],
      "metadata": {
        "id": "f8zr1w-QWMEV"
      },
      "id": "f8zr1w-QWMEV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTJXyB9dVSz-"
      },
      "outputs": [],
      "source": [
        "csv_files = sorted(glob.glob(\"urls_*.csv\"))\n",
        "print(f\" ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV: {len(csv_files)} ‡πÑ‡∏ü‡∏•‡πå\")\n",
        "\n",
        "dfs = []\n",
        "for fp in csv_files:\n",
        "    try:\n",
        "        df_temp = pd.read_csv(fp, on_bad_lines='skip')\n",
        "        dfs.append(df_temp)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Skipping file {fp}: {e}\")\n",
        "\n",
        "if not dfs:\n",
        "    raise ValueError(\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV ‡πÄ‡∏•‡∏¢ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå\")\n",
        "\n",
        "full_df = pd.concat(dfs, ignore_index=True)\n",
        "full_df = full_df[[\"url\", \"label\"]].dropna().drop_duplicates()\n",
        "\n",
        "print(f\" ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(full_df):,} ‡πÅ‡∏ñ‡∏ß\")\n",
        "\n",
        "SAMPLE_FRAC = 0.35\n",
        "if len(full_df) > 100000:\n",
        "    df = full_df.sample(frac=SAMPLE_FRAC, random_state=SEED)\n",
        "    print(f\"‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {SAMPLE_FRAC*100}% ‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç\")\n",
        "else:\n",
        "    df = full_df\n",
        "    print(\"info ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å\")\n",
        "\n",
        "print(f\" Final Dataset Size: {len(df):,} Rows\")\n",
        "del full_df\n",
        "import gc\n",
        "gc.collect()"
      ],
      "id": "lTJXyB9dVSz-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "IRRbA5MsWPHd"
      },
      "id": "IRRbA5MsWPHd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz_wMQ5EVSz_"
      },
      "outputs": [],
      "source": [
        "print(\" Processing data... (This might take a moment)\")\n",
        "\n",
        "urls = df[\"url\"].astype(str).tolist()\n",
        "labels_raw = df[\"label\"].values\n",
        "\n",
        "X_feat_raw = np.array([extract_features(u) for u in urls])\n",
        "num_features = X_feat_raw.shape[1]\n",
        "\n",
        "tokenizer = Tokenizer(char_level=True, lower=False, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(urls)\n",
        "sequences = tokenizer.texts_to_sequences(urls)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "MAX_LEN = 150\n",
        "X_seq = pad_sequences(sequences, maxlen=MAX_LEN, padding=\"post\")\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels_raw)\n",
        "\n",
        "X_seq_train, X_seq_test, X_feat_train, X_feat_test, y_train, y_test = train_test_split(\n",
        "    X_seq, X_feat_raw, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_feat_train = scaler.fit_transform(X_feat_train)\n",
        "X_feat_test = scaler.transform(X_feat_test)\n",
        "\n",
        "print(\" Preprocessing Complete!\")\n",
        "print(f\"   Train shape: {X_seq_train.shape}\")\n",
        "print(f\"   Vocab size: {vocab_size}\")"
      ],
      "id": "Xz_wMQ5EVSz_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Hybrid Model (Keras 3 Ready)"
      ],
      "metadata": {
        "id": "k7mqmK-HWSpy"
      },
      "id": "k7mqmK-HWSpy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHimloAZVSz_"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    url_input = Input(shape=(MAX_LEN,), dtype=\"int32\", name=\"url_input\")\n",
        "\n",
        "    x = Embedding(input_dim=vocab_size, output_dim=50)(url_input)\n",
        "\n",
        "    x = SpatialDropout1D(0.3)(x)\n",
        "\n",
        "    x = Bidirectional(LSTM(64, return_sequences=False))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    feat_input = Input(shape=(num_features,), dtype=\"float32\", name=\"features_input\")\n",
        "    y = Dense(64, activation=\"relu\")(feat_input)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Dropout(0.4)(y)\n",
        "    y = Dense(32, activation=\"relu\")(y)\n",
        "\n",
        "    merged = Concatenate()([x, y])\n",
        "    z = Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(merged)\n",
        "    z = Dropout(0.5)(z)\n",
        "\n",
        "    output = Dense(1, activation=\"sigmoid\")(z)\n",
        "\n",
        "    model = Model(inputs=[url_input, feat_input], outputs=output)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "id": "WHimloAZVSz_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "Le4biNp_WWBm"
      },
      "id": "Le4biNp_WWBm"
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2048\n",
        "EPOCHS = 20\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=4,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\",\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        \"best_hybrid_model.keras\",\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"üöÄ Start Training on {len(X_seq_train):,} samples...\")\n",
        "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
        "\n",
        "history = model.fit(\n",
        "    {\"url_input\": X_seq_train, \"features_input\": X_feat_train},\n",
        "    y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "gNERmtxDWXip"
      },
      "id": "gNERmtxDWXip",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate"
      ],
      "metadata": {
        "id": "RkQV4L3-WZTW"
      },
      "id": "RkQV4L3-WZTW"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Evaluating on Test Set...\")\n",
        "\n",
        "loss, accuracy = model.evaluate(\n",
        "    {\"url_input\": X_seq_test, \"features_input\": X_feat_test},\n",
        "    y_test,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1\n",
        ")\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "print(\" Saving files...\")\n",
        "model.save(\"final_hybrid_model.keras\")\n",
        "\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f: pickle.dump(tokenizer, f)\n",
        "with open(\"scaler.pkl\", \"wb\") as f: pickle.dump(scaler, f)\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f: pickle.dump(le, f)\n",
        "\n",
        "meta = {\"max_len\": MAX_LEN, \"num_features\": num_features}\n",
        "with open(\"hybrid_meta.pkl\", \"wb\") as f: pickle.dump(meta, f)\n",
        "\n",
        "print(\"All files saved successfully!\")\n",
        "\n",
        "from google.colab import files\n",
        "try:\n",
        "    files.download(\"best_hybrid_model.keras\")\n",
        "    files.download(\"tokenizer.pkl\")\n",
        "    files.download(\"scaler.pkl\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö† Auto-download failed (browser block?), please download manually from sidebar.\")"
      ],
      "metadata": {
        "id": "bhOmUzxAWacO"
      },
      "id": "bhOmUzxAWacO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Model + Tools"
      ],
      "metadata": {
        "id": "eJ2E5jTiWcRJ"
      },
      "id": "eJ2E5jTiWcRJ"
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"final_hybrid_model.keras\")\n",
        "\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f: pickle.dump(tokenizer, f)\n",
        "with open(\"scaler.pkl\", \"wb\") as f: pickle.dump(scaler, f)\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f: pickle.dump(le, f)\n",
        "\n",
        "meta = {\"max_len\": MAX_LEN, \"num_features\": num_features}\n",
        "with open(\"hybrid_meta.pkl\", \"wb\") as f: pickle.dump(meta, f)\n",
        "\n",
        "print(\"‚úî All files saved!\")\n"
      ],
      "metadata": {
        "id": "amlkZlCQWdS3"
      },
      "id": "amlkZlCQWdS3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto-Download All Output Files"
      ],
      "metadata": {
        "id": "SGE0nxQsWf4C"
      },
      "id": "SGE0nxQsWf4C"
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model(\"/content/final_hybrid_model.keras\")\n",
        "\n",
        "with open(\"/content/tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "with open(\"/content/scaler.pkl\", \"rb\") as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "with open(\"/content/label_encoder.pkl\", \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "with open(\"/content/hybrid_meta.pkl\", \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "\n",
        "MAX_LEN = meta[\"max_len\"]\n",
        "FEATURE_LEN = meta[\"num_features\"]\n",
        "\n",
        "print(\"‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!\")"
      ],
      "metadata": {
        "id": "UpKDkqvQWiNJ"
      },
      "id": "UpKDkqvQWiNJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def extract_features(url):\n",
        "    url = str(url)\n",
        "\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc\n",
        "    path = parsed.path\n",
        "\n",
        "    features = [\n",
        "        len(url),\n",
        "        len(domain),\n",
        "        len(path),\n",
        "        url.count('-'),\n",
        "        url.count('@'),\n",
        "        url.count('?'),\n",
        "        url.count('='),\n",
        "        url.count('.'),\n",
        "        url.count('/'),\n",
        "        1 if \"https\" in url else 0,\n",
        "        1 if \"@\" in url else 0,\n",
        "        1 if \"//\" in url else 0,\n",
        "        len(re.findall(r\"[0-9]\", url)),\n",
        "        len(re.findall(r\"[A-Z]\", url)),\n",
        "    ]\n",
        "\n",
        "    return np.array(features)\n"
      ],
      "metadata": {
        "id": "YOJ-0lG4dU1L"
      },
      "id": "YOJ-0lG4dU1L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(url):\n",
        "\n",
        "    seq = tokenizer.texts_to_sequences([url])\n",
        "    seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=MAX_LEN)\n",
        "\n",
        "    feat = extract_features(url)\n",
        "    feat = feat.reshape(1, -1)\n",
        "    feat = scaler.transform(feat)\n",
        "\n",
        "    return seq, feat\n"
      ],
      "metadata": {
        "id": "uc1a8InbdlQ-"
      },
      "id": "uc1a8InbdlQ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_url(url):\n",
        "    seq, feat = preprocess(url)\n",
        "\n",
        "    pred = model.predict(\n",
        "        {\"url_input\": seq, \"features_input\": feat},\n",
        "        verbose=0\n",
        "    )[0]\n",
        "\n",
        "    label = np.argmax(pred)\n",
        "    label_text = label_encoder.inverse_transform([label])[0]\n",
        "\n",
        "    confidence = float(np.max(pred)) * 100\n",
        "\n",
        "    return label_text, confidence, pred\n",
        "\n"
      ],
      "metadata": {
        "id": "5Jo0XJLTdnQr"
      },
      "id": "5Jo0XJLTdnQr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}